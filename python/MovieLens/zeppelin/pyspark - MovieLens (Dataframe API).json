{"paragraphs":[{"title":"Download and unpack MovieLens archive","text":"%sh\n\n# if [[ -d /tmp/ml-latest ]]; then\n#     rm -fr /tmp/ml-latest\n#     echo \"Removed existing /tmp/ml-latest folder...\"\n# fi\n\nwget -q -O /tmp/ml-latest.zip http://files.grouplens.org/datasets/movielens/ml-latest.zip \necho \"Dowloaded ml-latest.zip ...\"\n\nunzip /tmp/ml-latest.zip -d /tmp/\nrm /tmp/ml-latest.zip\necho \"Unpacked ml-latest.zip ...\"\n\ndir0=/tmp/datalake\ndir1=links\ndir2=movies\ndir3=ratings\ndir4=tags\n\nfor dir in $dir1 $dir2 $dir3 $dir4;do\n    # if [[ -d $dir0/$dir ]];then\n    #   rm -r $dir0/$dir\n    # fi\n    \n    mkdir -p $dir0/$dir\n\n    mv /tmp/ml-latest/$dir.csv $dir0/$dir/\n    echo \"Moved $dir.csv to $dir0/$dir/ ...\"\ndone\n\nrm -r /tmp/ml-latest\necho \"Datalake files created!\"","user":"anonymous","dateUpdated":"2020-03-31T08:23:05+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{"dir":""},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Dowloaded ml-latest.zip ...\nArchive:  /tmp/ml-latest.zip\n   creating: /tmp/ml-latest/\n  inflating: /tmp/ml-latest/links.csv  \n  inflating: /tmp/ml-latest/tags.csv  \n  inflating: /tmp/ml-latest/genome-tags.csv  \n  inflating: /tmp/ml-latest/ratings.csv  \n  inflating: /tmp/ml-latest/README.txt  \n  inflating: /tmp/ml-latest/genome-scores.csv  \n  inflating: /tmp/ml-latest/movies.csv  \nUnpacked ml-latest.zip ...\nMoved links.csv to /tmp/datalake/links/ ...\nMoved movies.csv to /tmp/datalake/movies/ ...\nMoved ratings.csv to /tmp/datalake/ratings/ ...\nMoved tags.csv to /tmp/datalake/tags/ ...\nDatalake files created!\n"}]},"apps":[],"jobName":"paragraph_1585064998264_1925293852","id":"20200319-112035_948031474","dateCreated":"2020-03-24T15:49:58+0000","dateStarted":"2020-03-31T08:23:06+0000","dateFinished":"2020-03-31T08:23:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3093"},{"text":"%sh\n\n# копируем файлы в HDFS\n\nhdfs dfs -mkdir -p /tmp/datalake/movies\nhdfs dfs -put -p /tmp/datalake/* /tmp/datalake/","user":"anonymous","dateUpdated":"2020-03-31T08:23:27+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1585064998278_-625152568","id":"20200319-212023_296231600","dateCreated":"2020-03-24T15:49:58+0000","dateStarted":"2020-03-31T08:23:27+0000","dateFinished":"2020-03-31T08:23:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3094"},{"title":"DataFrames","text":"%pyspark\n\ntuple_list = [('Timofey', 'Gavrilenko', 39), ('Elena', 'Bykova', 38)]\ndf = spark.createDataFrame(tuple_list)\ndf.collect()\n","user":"anonymous","dateUpdated":"2020-03-27T09:33:52+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(_1='Timofey', _2='Gavrilenko', _3=39), Row(_1='Elena', _2='Bykova', _3=38)]\n"}]},"apps":[],"jobName":"paragraph_1585065470491_-1914988140","id":"20200324-155750_1759270420","dateCreated":"2020-03-24T15:57:50+0000","dateStarted":"2020-03-27T09:33:52+0000","dateFinished":"2020-03-27T09:33:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3095"},{"text":"%pyspark\n\nspark.createDataFrame(tuple_list, ['first_name', 'last_name', 'age']).collect()\n","user":"anonymous","dateUpdated":"2020-03-24T16:07:21+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(first_name='Timofey', last_name='Gavrilenko', age=39), Row(first_name='Elena', last_name='Bykova', age=38)]\n"}]},"apps":[],"jobName":"paragraph_1585065600497_-1005177316","id":"20200324-160000_359793329","dateCreated":"2020-03-24T16:00:00+0000","dateStarted":"2020-03-24T16:07:21+0000","dateFinished":"2020-03-24T16:07:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3096"},{"text":"%pyspark\n\ndict_list = [{'first_name': r[0], 'last_name': r[1], 'age': r[2]} for r in tuple_list]\nprint(dict_list)\n\nspark.createDataFrame(dict_list).collect()","user":"anonymous","dateUpdated":"2020-03-27T09:37:39+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[{'first_name': 'Timofey', 'last_name': 'Gavrilenko', 'age': 39}, {'first_name': 'Elena', 'last_name': 'Bykova', 'age': 38}]\n/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n[Row(age=39, first_name='Timofey', last_name='Gavrilenko'), Row(age=38, first_name='Elena', last_name='Bykova')]\n"}]},"apps":[],"jobName":"paragraph_1585065682389_-1465915984","id":"20200324-160122_486123295","dateCreated":"2020-03-24T16:01:22+0000","dateStarted":"2020-03-27T09:37:40+0000","dateFinished":"2020-03-27T09:37:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3097"},{"text":"%pyspark\n\nrdd = sc.parallelize(tuple_list)\nspark.createDataFrame(rdd).collect()","user":"anonymous","dateUpdated":"2020-03-27T09:34:09+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(_1='Timofey', _2='Gavrilenko', _3=39), Row(_1='Elena', _2='Bykova', _3=38)]\n"}]},"apps":[],"jobName":"paragraph_1585066190514_2048472189","id":"20200324-160950_1507787164","dateCreated":"2020-03-24T16:09:50+0000","dateStarted":"2020-03-27T09:34:09+0000","dateFinished":"2020-03-27T09:34:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3098"},{"text":"%pyspark\n\nfrom pyspark.sql import Row\nPerson = Row('first_name', 'last_name', 'age')\nrdd_person = rdd.map(lambda r: Person(*r))\n\nrdd_person.collect()","user":"anonymous","dateUpdated":"2020-03-27T09:34:12+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(first_name='Timofey', last_name='Gavrilenko', age=39), Row(first_name='Elena', last_name='Bykova', age=38)]\n"}]},"apps":[],"jobName":"paragraph_1585066090633_96829376","id":"20200324-160810_1457508562","dateCreated":"2020-03-24T16:08:10+0000","dateStarted":"2020-03-27T09:34:12+0000","dateFinished":"2020-03-27T09:34:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3099"},{"text":"%pyspark\n\nspark.createDataFrame(rdd_person).collect()\n\n# rdd_person.toDF().collect()","user":"anonymous","dateUpdated":"2020-03-24T16:57:42+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(a='Timofey', b='Gavrilenko', c=39), Row(a='Elena', b='Bykova', c=38)]\n"}]},"apps":[],"jobName":"paragraph_1585066424257_-1587195457","id":"20200324-161344_1228866029","dateCreated":"2020-03-24T16:13:44+0000","dateStarted":"2020-03-24T16:57:16+0000","dateFinished":"2020-03-24T16:57:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3100"},{"text":"%pyspark\n\nspark.createDataFrame(rdd_person, \"first_name: string, b: int, c: integer, d: string\")","user":"anonymous","dateUpdated":"2020-03-27T09:44:48+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Py4JJavaError: An error occurred while calling o737.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 4 times, most recent failure: Lost task 0.3 in stage 16.0 (TID 61, cluster-bb90-m.us-central1-c.c.valiant-store-238312.internal, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 730, in prepare\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1389, in verify\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1368, in verify_struct\nValueError: Length of object (3) does not match with length of fields (4)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 730, in prepare\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1389, in verify\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1368, in verify_struct\nValueError: Length of object (3) does not match with length of fields (4)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o737.collectToPython.\\n', JavaObject id=o738), <traceback object at 0x7fc92e7d6908>)"}]},"apps":[],"jobName":"paragraph_1585069063231_1125371489","id":"20200324-165743_643032892","dateCreated":"2020-03-24T16:57:43+0000","dateStarted":"2020-03-27T09:44:19+0000","dateFinished":"2020-03-27T09:44:20+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:3101"},{"text":"%pyspark\n\nimport pandas as pd\n\n# pandas_df = pd.DataFrame(dict_list)\n# print(pandas_df)   # print(pandas_df.to_string())\n\n# spark_df = spark.createDataFrame(pandas_df)\n# print(spark_df)    # oops\n\nspark_df.collect()\nspark_df.show()","user":"anonymous","dateUpdated":"2020-03-27T09:38:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+----------+---+\n|first_name| last_name|age|\n+----------+----------+---+\n|   Timofey|Gavrilenko| 39|\n|     Elena|    Bykova| 38|\n+----------+----------+---+\n\n"}]},"apps":[],"jobName":"paragraph_1585068177757_1805127378","id":"20200324-164257_1947031718","dateCreated":"2020-03-24T16:42:57+0000","dateStarted":"2020-03-27T09:38:12+0000","dateFinished":"2020-03-27T09:38:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3102"},{"text":"%pyspark\n\nspark_df.createOrReplaceTempView(\"user\")\n\nspark.sql('select last_name, age from user where age > 38').show()","user":"anonymous","dateUpdated":"2020-03-24T17:03:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+---+\n| last_name|age|\n+----------+---+\n|Gavrilenko| 39|\n+----------+---+\n\n"}]},"apps":[],"jobName":"paragraph_1585068800105_1787358888","id":"20200324-165320_1322996616","dateCreated":"2020-03-24T16:53:20+0000","dateStarted":"2020-03-24T17:03:11+0000","dateFinished":"2020-03-24T17:03:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3103"},{"title":"Dataframe -> table","text":"%pyspark\n\ndf = spark.read.format('csv').option('header', 'true').load('hdfs:///tmp/datalake/movies/movies.csv')\ndf.printSchema()\n","user":"anonymous","dateUpdated":"2020-03-31T08:23:37+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- movieId: string (nullable = true)\n |-- title: string (nullable = true)\n |-- genres: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1585064998278_71439986","id":"20200319-132212_663298911","dateCreated":"2020-03-24T15:49:58+0000","dateStarted":"2020-03-31T08:23:37+0000","dateFinished":"2020-03-31T08:24:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3104"},{"text":"%pyspark\n\ndf.select('title', 'genres').show(10, truncate=False)\n","user":"anonymous","dateUpdated":"2020-03-27T09:52:51+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+\n|               title|              genres|\n+--------------------+--------------------+\n|    Toy Story (1995)|Adventure|Animati...|\n|      Jumanji (1995)|Adventure|Childre...|\n|Grumpier Old Men ...|      Comedy|Romance|\n|Waiting to Exhale...|Comedy|Drama|Romance|\n|Father of the Bri...|              Comedy|\n|         Heat (1995)|Action|Crime|Thri...|\n|      Sabrina (1995)|      Comedy|Romance|\n| Tom and Huck (1995)|  Adventure|Children|\n| Sudden Death (1995)|              Action|\n|    GoldenEye (1995)|Action|Adventure|...|\n+--------------------+--------------------+\nonly showing top 10 rows\n\n"}]},"apps":[],"jobName":"paragraph_1585064998278_-620715747","id":"20200319-175907_733148083","dateCreated":"2020-03-24T15:49:58+0000","dateStarted":"2020-03-27T09:52:17+0000","dateFinished":"2020-03-27T09:52:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3105"},{"text":"%pyspark\n\n# create our own udf-functions\n\nimport re\n\nget_title = spark.udf.register(\"extract_title_from_raw_title\", lambda x: re.search('(.+)[ ]+\\\\((\\\\d{4})\\\\)', x).group(1))\nget_year = spark.udf.register(\"get_year\", lambda x: re.search('(.+)[ ]+\\\\((\\\\d{4})\\\\)', x).group(2))\n","user":"anonymous","dateUpdated":"2020-03-31T08:43:22+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":88,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"title":"string","year":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1585064998281_1690463447","id":"20200319-182056_1135975523","dateCreated":"2020-03-24T15:49:58+0000","dateStarted":"2020-03-31T08:43:22+0000","dateFinished":"2020-03-31T08:43:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3106"},{"text":"%pyspark\n\ndf.select('title', get_title('title'), get_year('title')).show(5)","user":"anonymous","dateUpdated":"2020-03-27T09:59:26+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+---------------+\n|               title|    get_title(title)|get_year(title)|\n+--------------------+--------------------+---------------+\n|    Toy Story (1995)|           Toy Story|           1995|\n|      Jumanji (1995)|             Jumanji|           1995|\n|Grumpier Old Men ...|    Grumpier Old Men|           1995|\n|Waiting to Exhale...|   Waiting to Exhale|           1995|\n|Father of the Bri...|Father of the Bri...|           1995|\n+--------------------+--------------------+---------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1585072051973_1374572275","id":"20200324-174731_790593227","dateCreated":"2020-03-24T17:47:31+0000","dateStarted":"2020-03-27T09:59:26+0000","dateFinished":"2020-03-27T09:59:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3107"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import split\n\ndf.select(split('genres', '[|]')).show(truncate=False)","user":"anonymous","dateUpdated":"2020-03-31T08:43:32+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":102,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"split(Adventure|Animation|Children|Comedy|Fantasy, \\|)":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------------------------------------+\n|split(genres, [|])                               |\n+-------------------------------------------------+\n|[Adventure, Animation, Children, Comedy, Fantasy]|\n|[Adventure, Children, Fantasy]                   |\n|[Comedy, Romance]                                |\n|[Comedy, Drama, Romance]                         |\n|[Comedy]                                         |\n|[Action, Crime, Thriller]                        |\n|[Comedy, Romance]                                |\n|[Adventure, Children]                            |\n|[Action]                                         |\n|[Action, Adventure, Thriller]                    |\n|[Comedy, Drama, Romance]                         |\n|[Comedy, Horror]                                 |\n|[Adventure, Animation, Children]                 |\n|[Drama]                                          |\n|[Action, Adventure, Romance]                     |\n|[Crime, Drama]                                   |\n|[Drama, Romance]                                 |\n|[Comedy]                                         |\n|[Comedy]                                         |\n|[Action, Comedy, Crime, Drama, Thriller]         |\n+-------------------------------------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1585064998281_234746134","id":"20200319-181722_1321559168","dateCreated":"2020-03-24T15:49:58+0000","dateStarted":"2020-03-31T08:43:32+0000","dateFinished":"2020-03-31T08:43:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3108"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import split, explode\n\ndf_normalized = df.select( \\\n                      get_title('title').alias('title'), \\\n                      get_year('title').alias('year'), \\\n                      explode(split('genres', '\\\\|')).alias('genre') \\\n                )\n\ndf_normalized.show()","user":"anonymous","dateUpdated":"2020-03-31T08:43:35+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"title":"string","year":"string","genre":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+----+---------+\n|               title|year|    genre|\n+--------------------+----+---------+\n|           Toy Story|1995|Adventure|\n|           Toy Story|1995|Animation|\n|           Toy Story|1995| Children|\n|           Toy Story|1995|   Comedy|\n|           Toy Story|1995|  Fantasy|\n|             Jumanji|1995|Adventure|\n|             Jumanji|1995| Children|\n|             Jumanji|1995|  Fantasy|\n|    Grumpier Old Men|1995|   Comedy|\n|    Grumpier Old Men|1995|  Romance|\n|   Waiting to Exhale|1995|   Comedy|\n|   Waiting to Exhale|1995|    Drama|\n|   Waiting to Exhale|1995|  Romance|\n|Father of the Bri...|1995|   Comedy|\n|                Heat|1995|   Action|\n|                Heat|1995|    Crime|\n|                Heat|1995| Thriller|\n|             Sabrina|1995|   Comedy|\n|             Sabrina|1995|  Romance|\n|        Tom and Huck|1995|Adventure|\n+--------------------+----+---------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1585064998281_2143215742","id":"20200319-181426_1609282113","dateCreated":"2020-03-24T15:49:58+0000","dateStarted":"2020-03-31T08:43:35+0000","dateFinished":"2020-03-31T08:43:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3109"},{"text":"%pyspark\n\n# using standard functions only\n\nfrom pyspark.sql.functions import split, explode, regexp_extract\n\ndf_normalized = df.select( \\\n                      df.movieId, \\\n                      regexp_extract(df.title, '(.+)[ ]+[(](\\\\d{4})[)]', 1).alias('title'), \\\n                      regexp_extract(df.title, '(.+)[ ]+[(](\\\\d{4})[)]', 2).alias('year'), \\\n                      explode(split(df.genres, '\\\\|')).alias('genre') \\\n                )\n\ndf_normalized.show()","user":"anonymous","dateUpdated":"2020-03-31T08:43:56+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+--------------------+----+---------+\n|movieId|               title|year|    genre|\n+-------+--------------------+----+---------+\n|      1|           Toy Story|1995|Adventure|\n|      1|           Toy Story|1995|Animation|\n|      1|           Toy Story|1995| Children|\n|      1|           Toy Story|1995|   Comedy|\n|      1|           Toy Story|1995|  Fantasy|\n|      2|             Jumanji|1995|Adventure|\n|      2|             Jumanji|1995| Children|\n|      2|             Jumanji|1995|  Fantasy|\n|      3|    Grumpier Old Men|1995|   Comedy|\n|      3|    Grumpier Old Men|1995|  Romance|\n|      4|   Waiting to Exhale|1995|   Comedy|\n|      4|   Waiting to Exhale|1995|    Drama|\n|      4|   Waiting to Exhale|1995|  Romance|\n|      5|Father of the Bri...|1995|   Comedy|\n|      6|                Heat|1995|   Action|\n|      6|                Heat|1995|    Crime|\n|      6|                Heat|1995| Thriller|\n|      7|             Sabrina|1995|   Comedy|\n|      7|             Sabrina|1995|  Romance|\n|      8|        Tom and Huck|1995|Adventure|\n+-------+--------------------+----+---------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1585073975078_-417848666","id":"20200324-181935_2090427844","dateCreated":"2020-03-24T18:19:35+0000","dateStarted":"2020-03-31T08:43:56+0000","dateFinished":"2020-03-31T08:43:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3110"},{"text":"%pyspark\n\n# using sql-like filter\n\ndf_filtered = df_normalized.filter('(genres in (\"Comedy\", \"Drama\")) and (year between 2000 and 2005) and (lower(title) like \"%love%\")')\n\ndf_filtered.show()","user":"anonymous","dateUpdated":"2020-03-31T08:44:00+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+--------------------+----+------+\n|movieId|               title|year| genre|\n+-------+--------------------+----+------+\n|   5129|        Big Bad Love|2001| Drama|\n|   5321|Triumph of Love, The|2001|Comedy|\n|   5327|In Praise of Love...|2001| Drama|\n|   5559|Mad Love (Juana l...|2001| Drama|\n|   5790|Food of Love (Man...|2002| Drama|\n|   6002|           Love Liza|2002| Drama|\n|   6018|Kira's Reason: A ...|2001| Drama|\n|   6489|Loco Love (Mi Cas...|2002|Comedy|\n|   6943|Love Forbidden (D...|2002| Drama|\n|   8956|       Enduring Love|2004| Drama|\n|  27888|When Will I Be Loved|2004| Drama|\n|  30848|Love Song for Bob...|2004| Drama|\n|  45346|           Lollilove|2004|Comedy|\n|  56614|Regular Lovers (A...|2005| Drama|\n|  82061|To Paint or Make ...|2005|Comedy|\n|  87683| Unexpected Love, An|2003| Drama|\n| 136487|Sexaholix... A Lo...|2002|Comedy|\n| 140884|    The Road To Love|2001| Drama|\n| 151971|Love Ke Liye Kuch...|2001|Comedy|\n| 152346|EK Chotti Si Love...|2002| Drama|\n+-------+--------------------+----+------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1585073368914_-1810309771","id":"20200324-180928_1728027248","dateCreated":"2020-03-24T18:09:28+0000","dateStarted":"2020-03-31T08:44:00+0000","dateFinished":"2020-03-31T08:44:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3111"},{"text":"%pyspark\n\ndf_filtered_ids = df_normalized.filter(df_normalized.genre.isin(['Comedy', 'Drama'])) \\\n                           .filter(df_normalized.year.between(2000, 2005)) \\\n                           .filter(df_normalized.title.rlike('(?i)love')) \\\n                           .select(df.movieId) \\\n                           .distinct()\n\ndf_filtered_ids.show()","user":"anonymous","dateUpdated":"2020-03-31T08:44:08+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+\n|movieId|\n+-------+\n|   5450|\n|   5321|\n|  71707|\n| 176623|\n|   6757|\n|  27658|\n| 169768|\n|  27888|\n|  42393|\n|   4144|\n| 128872|\n|   6367|\n|  64754|\n|   4235|\n| 176591|\n| 179431|\n|   6384|\n|   5673|\n|   5886|\n|   6897|\n+-------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1585074715813_-965967956","id":"20200324-183155_1972017879","dateCreated":"2020-03-24T18:31:55+0000","dateStarted":"2020-03-31T08:44:08+0000","dateFinished":"2020-03-31T08:44:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3112"},{"text":"%pyspark\n\ndf_filtered = df_normalized.join(df_filtered_ids, df_normalized.movieId == df_filtered_ids.movieId) \\\n                           .select('title', 'year', 'genre')\n\ndf_filtered.show()","user":"anonymous","dateUpdated":"2020-03-31T08:46:13+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+----+--------+\n|               title|year|   genre|\n+--------------------+----+--------+\n| Love and Basketball|2000|   Drama|\n| Love and Basketball|2000| Romance|\n|Love's Labour's Lost|2000|  Comedy|\n|Love's Labour's Lost|2000| Romance|\n|          Love & Sex|2000|  Comedy|\n|          Love & Sex|2000|   Drama|\n|          Love & Sex|2000| Romance|\n|In the Mood For L...|2000|   Drama|\n|In the Mood For L...|2000| Romance|\n|Amores Perros (Lo...|2000|   Drama|\n|Amores Perros (Lo...|2000|Thriller|\n|Thomas in Love (T...|2000|  Comedy|\n|Thomas in Love (T...|2000|   Drama|\n|        Big Bad Love|2001|   Drama|\n|Triumph of Love, The|2001|  Comedy|\n|In Praise of Love...|2001|   Drama|\n|    Lovely & Amazing|2001|  Comedy|\n|    Lovely & Amazing|2001|   Drama|\n|    Lovely & Amazing|2001| Romance|\n|Mad Love (Juana l...|2001|   Drama|\n+--------------------+----+--------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1585076213145_1204045999","id":"20200324-185653_1952029790","dateCreated":"2020-03-24T18:56:53+0000","dateStarted":"2020-03-31T08:46:13+0000","dateFinished":"2020-03-31T08:46:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3113"},{"text":"%pyspark\n\n# получение результатов требует довольно длительного времени. посмотрим план выполнения\n\ndf_filtered.explain()\n\n# для сравнения, см DAG выполнения в Spark UI","user":"anonymous","dateUpdated":"2020-03-24T20:25:24+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"== Physical Plan ==\n*(5) Project [title#557, year#558, genre#560]\n+- *(5) BroadcastHashJoin [movieId#248], [movieId#809], Inner, BuildRight\n   :- *(5) Project [movieId#248, regexp_extract(title#249, (.+)[ ]+[(](\\d{4})[)], 1) AS title#557, regexp_extract(title#249, (.+)[ ]+[(](\\d{4})[)], 2) AS year#558, genre#560]\n   :  +- Generate explode(split(genres#250, \\|)), [movieId#248, title#249], false, [genre#560]\n   :     +- *(1) Project [movieId#248, title#249, genres#250]\n   :        +- *(1) Filter isnotnull(movieId#248)\n   :           +- *(1) FileScan csv [movieId#248,title#249,genres#250] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://cluster-bb0f-m/tmp/datalake/movies/movies.csv], PartitionFilters: [], PushedFilters: [IsNotNull(movieId)], ReadSchema: struct<movieId:string,title:string,genres:string>\n   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n      +- *(4) HashAggregate(keys=[movieId#809], functions=[])\n         +- Exchange hashpartitioning(movieId#809, 200)\n            +- *(3) HashAggregate(keys=[movieId#809], functions=[])\n               +- *(3) Project [movieId#809]\n                  +- *(3) Filter genre#560 IN (Comedy,Drama)\n                     +- Generate explode(split(genres#811, \\|)), [movieId#809], false, [genre#560]\n                        +- *(2) Project [movieId#809, genres#811]\n                           +- *(2) Filter ((((cast(regexp_extract(title#810, (.+)[ ]+[(](\\d{4})[)], 2) as int) >= 2000) && (cast(regexp_extract(title#810, (.+)[ ]+[(](\\d{4})[)], 2) as int) <= 2005)) && regexp_extract(title#810, (.+)[ ]+[(](\\d{4})[)], 1) RLIKE (?i)love) && isnotnull(movieId#809))\n                              +- *(2) FileScan csv [movieId#809,title#810,genres#811] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://cluster-bb0f-m/tmp/datalake/movies/movies.csv], PartitionFilters: [], PushedFilters: [IsNotNull(movieId)], ReadSchema: struct<movieId:string,title:string,genres:string>\n"}]},"apps":[],"jobName":"paragraph_1585081381100_112351146","id":"20200324-202301_848047099","dateCreated":"2020-03-24T20:23:01+0000","dateStarted":"2020-03-24T20:23:56+0000","dateFinished":"2020-03-24T20:23:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3114"},{"text":"%pyspark\n\n# for comparison with SQL version\n\ndf_filtered = spark.sql(\"\"\"\nwith cte_reg\nas\n(\n    select '(.+)[ ]+[(](\\\\\\d{4})[)]' regexp\n),\ncte_wished_genres\nas\n(\n    select explode(split('Comedy|Drama', '[|]')) as genre\n),\ncte_split\nas\n(\n    select m.movieId,\n           regexp_extract(m.title, r.regexp, 1) as title,\n           regexp_extract(m.title, r.regexp, 2) as year,\n           explode(split(m.genres, '[|]')) as genre\n    from cte_reg r \n    cross join movies m\n),\ncte_filtered_ids\nas\n(\n    select s.movieId\n    from cte_split s\n    join cte_wished_genres wg on s.genre = wg.genre\n    where s.year between 2000 and 2005\n        and lower(s.title) like '%love%'\n)\nselect s.title,\n       s.year,\n       s.genre\nfrom cte_split s\nwhere s.movieId in (select movieId from cte_filtered_ids)\n\"\"\")","user":"anonymous","dateUpdated":"2020-03-24T19:07:18+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585064998281_-1244985391","id":"20200319-230316_211226499","dateCreated":"2020-03-24T15:49:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3115"},{"text":"%pyspark\n\ndf_filtered.write.mode('overwrite') \\\n    .format('csv') \\\n    .partitionBy('genre') \\\n    .option('header', 'true') \\\n    .option('delimiter', ';') \\\n    .option('encoding', 'utf-8') \\\n    .option('path', 'hdfs:///tmp/movies/map-filter') \\\n    .saveAsTable('movies_mapped_and_filtered')\n    \n# дополнительно можно было создать бакеты, например 10шт по колонке year: .bucketBy(10, 'year') \\\n\n# в будущем можно будет обновлять файлы только в тех партициях (фолдерах) на диске, которые соответствуют данным в датафрейме\n# spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n# df_filtered.write.insertInto(\"movies_mapped_and_filtered\", overwrite=True)","user":"anonymous","dateUpdated":"2020-03-31T08:46:53+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1585064998283_796654304","id":"20200320-063200_1353818257","dateCreated":"2020-03-24T15:49:58+0000","dateStarted":"2020-03-31T08:46:53+0000","dateFinished":"2020-03-31T08:47:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3116"},{"text":"%sh\n\nhdfs dfs -ls /tmp/movies/map-filter","user":"anonymous","dateUpdated":"2020-03-24T20:29:19+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 16 items\n-rw-r--r--   2 zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/_SUCCESS\ndrwxr-xr-x   - zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/genre=Action\ndrwxr-xr-x   - zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/genre=Adventure\ndrwxr-xr-x   - zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/genre=Animation\ndrwxr-xr-x   - zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/genre=Comedy\ndrwxr-xr-x   - zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/genre=Crime\ndrwxr-xr-x   - zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/genre=Drama\ndrwxr-xr-x   - zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/genre=Fantasy\ndrwxr-xr-x   - zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/genre=Horror\ndrwxr-xr-x   - zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/genre=Musical\ndrwxr-xr-x   - zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/genre=Mystery\ndrwxr-xr-x   - zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/genre=Romance\ndrwxr-xr-x   - zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/genre=Sci-Fi\ndrwxr-xr-x   - zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/genre=Thriller\ndrwxr-xr-x   - zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/genre=War\ndrwxr-xr-x   - zeppelin hadoop          0 2020-03-24 20:29 /tmp/movies/map-filter/genre=Western\n"}]},"apps":[],"jobName":"paragraph_1585077176981_1310797662","id":"20200324-191256_1434776807","dateCreated":"2020-03-24T19:12:56+0000","dateStarted":"2020-03-24T20:29:19+0000","dateFinished":"2020-03-24T20:29:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3117"},{"text":"%sh\n\nhdfs dfs -ls /tmp/movies/map-filter/genre=Action\n\nhdfs dfs -cat /tmp/movies/map-filter/genre=Action/*","user":"anonymous","dateUpdated":"2020-03-24T20:31:40+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 1 items\n-rw-r--r--   2 zeppelin hadoop        107 2020-03-24 20:29 /tmp/movies/map-filter/genre=Action/part-00000-98d8709d-c38c-4672-9444-9db303c5bc04.c000.csv\ntitle;year\nHero: Love Story of a Spy, The;2003\nLucky: No Time For Love;2005\nFaust: Love of the Damned;2001\n"}]},"apps":[],"jobName":"paragraph_1585081894691_1802596765","id":"20200324-203134_1589603483","dateCreated":"2020-03-24T20:31:34+0000","dateStarted":"2020-03-24T20:31:41+0000","dateFinished":"2020-03-24T20:31:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3118"},{"text":"%pyspark\n\ndf = spark.read \\\n    .format('csv') \\\n    .option('header', 'true') \\\n    .option('delimiter', ';') \\\n    .option('encoding', 'utf-8') \\\n    .option('path', 'hdfs:///tmp/movies/map-filter') \\\n    .load()\n\ndf.show(10)\n","user":"anonymous","dateUpdated":"2020-03-31T08:55:35+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+----+-----+\n|               title|year|genre|\n+--------------------+----+-----+\n| Love and Basketball|2000|Drama|\n|          Love & Sex|2000|Drama|\n|In the Mood For L...|2000|Drama|\n|Amores Perros (Lo...|2000|Drama|\n|Thomas in Love (T...|2000|Drama|\n|        Big Bad Love|2001|Drama|\n|In Praise of Love...|2001|Drama|\n|    Lovely & Amazing|2001|Drama|\n|Mad Love (Juana l...|2001|Drama|\n|    Punch-Drunk Love|2002|Drama|\n+--------------------+----+-----+\nonly showing top 10 rows\n\n"}]},"apps":[],"jobName":"paragraph_1585077276713_580533427","id":"20200324-191436_1132232750","dateCreated":"2020-03-24T19:14:36+0000","dateStarted":"2020-03-31T08:55:36+0000","dateFinished":"2020-03-31T08:55:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3119"},{"text":"%pyspark\n\ndf.filter(df.genre.isin(['Romance'])).explain()","user":"anonymous","dateUpdated":"2020-03-24T20:29:51+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"== Physical Plan ==\n*(1) FileScan csv [title#1125,year#1126,genre#1127] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://cluster-bb0f-m/tmp/movies/map-filter], PartitionCount: 1, PartitionFilters: [isnotnull(genre#1127), (genre#1127 = Romance)], PushedFilters: [], ReadSchema: struct<title:string,year:string>\n"}]},"apps":[],"jobName":"paragraph_1585081167330_277718825","id":"20200324-201927_2079201761","dateCreated":"2020-03-24T20:19:27+0000","dateStarted":"2020-03-24T20:29:52+0000","dateFinished":"2020-03-24T20:29:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3120"},{"text":"%md\n\n## Настройка mysql для последующей работы через jdbc (один раз)\n\nСбрасываем пароль пользователя root, создаем БД movies и пользователя test для работы с ней","user":"anonymous","dateUpdated":"2020-03-24T15:49:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585064998284_1550816475","id":"20200320-082835_124290840","dateCreated":"2020-03-24T15:49:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3121"},{"text":"%md\n### run in first terminal\n\nsudo systemctl stop mysql\nsudo mysqld --user=mysql --skip-grant-tables\n","user":"anonymous","dateUpdated":"2020-03-24T15:49:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585064998284_1351423264","id":"20200320-081110_314903082","dateCreated":"2020-03-24T15:49:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3122"},{"text":"%md\n\n### run in a parallel terminal\n\nmysql -u root\n\nflush privileges;\nupdate mysql.user set password=password('root_password') where user='root';\ncreate user 'test'@'%' identified by 'test_password';\ncreate databases movies;\ngrant all privileges on movies.* to test@'%';\nflush privileges;\nexit\n\nsudo kill -term <pid>\nsudo systemctl start mysql","user":"anonymous","dateUpdated":"2020-03-24T15:49:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585064998284_375096504","id":"20200320-081834_378481033","dateCreated":"2020-03-24T15:49:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3123"},{"text":"%pyspark\n\n# запись нормализованной и отфильтрованной таблицы в mysql\n\ndf_filtered.write \\\n  .mode('overwrite') \\\n  .format(\"jdbc\") \\\n  .option(\"url\", \"jdbc:mysql://cluster-3e91-m\") \\\n  .option(\"dbtable\", \"movies.movies\") \\\n  .option(\"user\", \"test\") \\\n  .option(\"password\", \"test_password\") \\\n  .save()","user":"anonymous","dateUpdated":"2020-03-31T09:35:22+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1585064998285_-1741871122","id":"20200320-064109_2125691155","dateCreated":"2020-03-24T15:49:58+0000","dateStarted":"2020-03-31T09:35:23+0000","dateFinished":"2020-03-31T09:35:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3124"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import struct, col, to_json\n\ndf_with_jsons = df_filtered.select('genre', to_json(struct(col('title'), col('year'))).alias('movie'))","user":"anonymous","dateUpdated":"2020-03-31T09:42:37+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1585082239803_-281722300","id":"20200324-203719_913616155","dateCreated":"2020-03-24T20:37:19+0000","dateStarted":"2020-03-31T09:42:37+0000","dateFinished":"2020-03-31T09:42:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3125"},{"text":"%pyspark\ndf_with_jsons.show(truncate=False)","user":"anonymous","dateUpdated":"2020-03-31T09:43:17+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+----------------------------------------------------------------+\n|genre   |movie                                                           |\n+--------+----------------------------------------------------------------+\n|Drama   |{\"title\":\"Love and Basketball\",\"year\":\"2000\"}                   |\n|Romance |{\"title\":\"Love and Basketball\",\"year\":\"2000\"}                   |\n|Comedy  |{\"title\":\"Love's Labour's Lost\",\"year\":\"2000\"}                  |\n|Romance |{\"title\":\"Love's Labour's Lost\",\"year\":\"2000\"}                  |\n|Comedy  |{\"title\":\"Love & Sex\",\"year\":\"2000\"}                            |\n|Drama   |{\"title\":\"Love & Sex\",\"year\":\"2000\"}                            |\n|Romance |{\"title\":\"Love & Sex\",\"year\":\"2000\"}                            |\n|Drama   |{\"title\":\"In the Mood For Love (Fa yeung nin wa)\",\"year\":\"2000\"}|\n|Romance |{\"title\":\"In the Mood For Love (Fa yeung nin wa)\",\"year\":\"2000\"}|\n|Drama   |{\"title\":\"Amores Perros (Love's a Bitch)\",\"year\":\"2000\"}        |\n|Thriller|{\"title\":\"Amores Perros (Love's a Bitch)\",\"year\":\"2000\"}        |\n|Comedy  |{\"title\":\"Thomas in Love (Thomas est Amoureux)\",\"year\":\"2000\"}  |\n|Drama   |{\"title\":\"Thomas in Love (Thomas est Amoureux)\",\"year\":\"2000\"}  |\n|Drama   |{\"title\":\"Big Bad Love\",\"year\":\"2001\"}                          |\n|Comedy  |{\"title\":\"Triumph of Love, The\",\"year\":\"2001\"}                  |\n|Drama   |{\"title\":\"In Praise of Love (Éloge de l'amour)\",\"year\":\"2001\"}  |\n|Comedy  |{\"title\":\"Lovely & Amazing\",\"year\":\"2001\"}                      |\n|Drama   |{\"title\":\"Lovely & Amazing\",\"year\":\"2001\"}                      |\n|Romance |{\"title\":\"Lovely & Amazing\",\"year\":\"2001\"}                      |\n|Drama   |{\"title\":\"Mad Love (Juana la Loca)\",\"year\":\"2001\"}              |\n+--------+----------------------------------------------------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1585647771525_-23473597","id":"20200331-094251_138362858","dateCreated":"2020-03-31T09:42:51+0000","dateStarted":"2020-03-31T09:43:17+0000","dateFinished":"2020-03-31T09:43:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3126"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import collect_list\n\ndf_result = df_with_jsons.groupBy('genre').agg(collect_list('movie').alias('movies'))\n\ndf_result.first()","user":"anonymous","dateUpdated":"2020-03-31T09:44:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Row(genre='Crime', movies=['{\"title\":\"Love, Honour and Obey\",\"year\":\"2000\"}', '{\"title\":\"Love the Hard Way\",\"year\":\"2001\"}', '{\"title\":\"Demonlover\",\"year\":\"2002\"}', '{\"title\":\"Consequences of Love, The (Conseguenze dell\\'amore, Le)\",\"year\":\"2004\"}', '{\"title\":\"Love\",\"year\":\"2005\"}', '{\"title\":\"Bad Luck Love\",\"year\":\"2000\"}'])\n"}]},"apps":[],"jobName":"paragraph_1585085129647_-632957452","id":"20200324-212529_1217380460","dateCreated":"2020-03-24T21:25:29+0000","dateStarted":"2020-03-31T09:44:11+0000","dateFinished":"2020-03-31T09:44:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3127"},{"text":"%pyspark\n\ndf_result.write \\\n  .mode('overwrite') \\\n  .format('json') \\\n  .option('path', '/tmp/movies/map-filter-reduce') \\\n  .save()","user":"anonymous","dateUpdated":"2020-03-31T09:44:35+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"genre":"string","movie":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1585064998285_-1723588850","id":"20200321-095912_1827079652","dateCreated":"2020-03-24T15:49:58+0000","dateStarted":"2020-03-31T09:44:35+0000","dateFinished":"2020-03-31T09:44:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3128"},{"text":"%sh\n\nhdfs dfs -ls /tmp/movies/map-filter-reduce","user":"anonymous","dateUpdated":"2020-03-24T21:41:02+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 17 items\n-rw-r--r--   2 zeppelin hadoop          0 2020-03-24 21:40 /tmp/movies/map-filter-reduce/_SUCCESS\n-rw-r--r--   2 zeppelin hadoop          0 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00000-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n-rw-r--r--   2 zeppelin hadoop        370 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00008-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n-rw-r--r--   2 zeppelin hadoop       3414 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00010-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n-rw-r--r--   2 zeppelin hadoop        480 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00014-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n-rw-r--r--   2 zeppelin hadoop        208 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00033-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n-rw-r--r--   2 zeppelin hadoop       4544 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00045-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n-rw-r--r--   2 zeppelin hadoop        154 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00050-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n-rw-r--r--   2 zeppelin hadoop        174 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00076-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n-rw-r--r--   2 zeppelin hadoop        131 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00106-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n-rw-r--r--   2 zeppelin hadoop        426 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00113-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n-rw-r--r--   2 zeppelin hadoop        339 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00118-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n-rw-r--r--   2 zeppelin hadoop        140 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00160-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n-rw-r--r--   2 zeppelin hadoop        141 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00169-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n-rw-r--r--   2 zeppelin hadoop       2584 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00178-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n-rw-r--r--   2 zeppelin hadoop        219 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00183-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n-rw-r--r--   2 zeppelin hadoop        148 2020-03-24 21:40 /tmp/movies/map-filter-reduce/part-00193-f975e488-4a3a-4108-9c39-d085782b1ef1-c000.json\n"}]},"apps":[],"jobName":"paragraph_1585064998288_-2106401759","id":"20200322-192253_1273614096","dateCreated":"2020-03-24T15:49:58+0000","dateStarted":"2020-03-24T21:41:02+0000","dateFinished":"2020-03-24T21:41:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3129"},{"text":"%sh\n\nhdfs dfs -cat /tmp/movies/map-filter-reduce/*008*","user":"anonymous","dateUpdated":"2020-03-31T09:46:14+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"genre\":\"Crime\",\"movies\":[\"{\\\"title\\\":\\\"Love, Honour and Obey\\\",\\\"year\\\":\\\"2000\\\"}\",\"{\\\"title\\\":\\\"Love the Hard Way\\\",\\\"year\\\":\\\"2001\\\"}\",\"{\\\"title\\\":\\\"Demonlover\\\",\\\"year\\\":\\\"2002\\\"}\",\"{\\\"title\\\":\\\"Consequences of Love, The (Conseguenze dell'amore, Le)\\\",\\\"year\\\":\\\"2004\\\"}\",\"{\\\"title\\\":\\\"Love\\\",\\\"year\\\":\\\"2005\\\"}\",\"{\\\"title\\\":\\\"Bad Luck Love\\\",\\\"year\\\":\\\"2000\\\"}\"]}\n"}]},"apps":[],"jobName":"paragraph_1585064998288_-1833599449","id":"20200322-192427_524029139","dateCreated":"2020-03-24T15:49:58+0000","dateStarted":"2020-03-31T09:46:15+0000","dateFinished":"2020-03-31T09:46:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3130"},{"title":"Full DataFrame API-code","text":"%pyspark\n\ndf = spark.read.format('csv').option('header', 'true').load('hdfs:///tmp/datalake/movies/movies.csv')\n\n#####################################################################################################\n\nfrom pyspark.sql.functions import split, explode, regexp_extract\n\ndf_normalized = df.select( \\\n                      df.movieId, \\\n                      regexp_extract(df.title, '(.+)[ ]+[(](\\\\d{4})[)]', 1).alias('title'), \\\n                      regexp_extract(df.title, '(.+)[ ]+[(](\\\\d{4})[)]', 2).alias('year'), \\\n                      explode(split(df.genres, '\\\\|')).alias('genre') \\\n                )\n\n\ndf_filtered_ids = df_normalized.filter(df_normalized.genre.isin(['Comedy', 'Drama'])) \\\n                           .filter(df_normalized.year.between(2000, 2005)) \\\n                           .filter(df_normalized.title.rlike('(?i)love')) \\\n                           .select(df.movieId) \\\n                           .distinct()\n\ndf_filtered = df_normalized.join(df_filtered_ids, df_normalized.movieId == df_filtered_ids.movieId) \\\n                           .select('title', 'year', 'genre')\n\n#####################################################################################################\n\nfrom pyspark.sql.functions import struct, col, to_json, collect_list\n\ndf_with_jsons = df_filtered.select('genre', to_json(struct(col('title'), col('year'))).alias('movie'))\n\n#####################################################################################################\n\ndf_result = df_with_jsons.groupBy('genre').agg(collect_list('movie').alias('movies'))\n\ndf_result.write \\\n  .mode('overwrite') \\\n  .format('json') \\\n  .option('path', '/tmp/movies/map-filter-reduce') \\\n  .save()","user":"anonymous","dateUpdated":"2020-03-31T09:48:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1585064998290_1265844294","id":"20200322-211834_1593929812","dateCreated":"2020-03-24T15:49:58+0000","dateStarted":"2020-03-31T09:48:05+0000","dateFinished":"2020-03-31T09:48:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3131"},{"text":"%sh\n\nhdfs dfs -ls /tmp/movies/map-filter-reduce","user":"anonymous","dateUpdated":"2020-03-24T21:51:30+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 17 items\n-rw-r--r--   2 zeppelin hadoop          0 2020-03-24 21:50 /tmp/movies/map-filter-reduce/_SUCCESS\n-rw-r--r--   2 zeppelin hadoop          0 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00000-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n-rw-r--r--   2 zeppelin hadoop        370 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00008-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n-rw-r--r--   2 zeppelin hadoop       3414 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00010-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n-rw-r--r--   2 zeppelin hadoop        480 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00014-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n-rw-r--r--   2 zeppelin hadoop        208 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00033-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n-rw-r--r--   2 zeppelin hadoop       4544 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00045-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n-rw-r--r--   2 zeppelin hadoop        154 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00050-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n-rw-r--r--   2 zeppelin hadoop        174 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00076-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n-rw-r--r--   2 zeppelin hadoop        131 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00106-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n-rw-r--r--   2 zeppelin hadoop        426 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00113-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n-rw-r--r--   2 zeppelin hadoop        339 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00118-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n-rw-r--r--   2 zeppelin hadoop        140 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00160-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n-rw-r--r--   2 zeppelin hadoop        141 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00169-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n-rw-r--r--   2 zeppelin hadoop       2584 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00178-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n-rw-r--r--   2 zeppelin hadoop        219 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00183-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n-rw-r--r--   2 zeppelin hadoop        148 2020-03-24 21:50 /tmp/movies/map-filter-reduce/part-00193-ffc1a51b-49de-4470-83f5-8c6e0dc726a4-c000.json\n"}]},"apps":[],"jobName":"paragraph_1585086635562_-1184480358","id":"20200324-215035_2140354469","dateCreated":"2020-03-24T21:50:35+0000","dateStarted":"2020-03-24T21:51:30+0000","dateFinished":"2020-03-24T21:51:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3132"}],"name":"pyspark - MovieLens (Dataframe API)","id":"2F68ZJ9YF","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}